# Core assignment: Pilot + Evaluation Prep [+4.5] Group

**Phase** : Evaluation
**Due date** : Friday, March 7 at 11:59pm PT
**The goal** of this stage of the project is to evaluate your high-fidelity prototype with potential users.
**Course learning objectives** this assignment facilitates: (1) Create an interactive system grounded in user
research, iterative prototyping, and evaluation; (2) Explore and express complex problems, design
choices; and (3) Reflect on what you know, don’t know, and how to learn what you don’t know.
**Grading** : This is a group assignment. All members of the group are expected to participate fully. Each
group will receive one grade. / Every member will receive the same number of points.
**What to submit** : Add a .MD file in your Github repo with the responses to the following questions.
**Submit the github repo link to BruinLearn. Only one person needs to submit for the group.**

## Pilot [+1]

1. [+0.5] Present the pilot user with a brief statement of the scenario and task. Ask the pilot user to
    complete the task. Note: You might feel (very) nervous that something will break. That is OK. It's
    ok for the pilot user to break things as they test out your system. Be prepared to restart/recover
    your system when things break. Note what happened step by step. Include 0.5-1p of notes on one
    pilot user. Additionally, summarize in a few sentences: What happened? Why? What changes do
    you need to make to your system before the next pilot?
We have done 6 different pilot tests with different users. We involved 3 members from our class during
the in class activity, who had more context about our project, and 3 members who are outside of the
course. We have included notes in the github folder with information about each pilot test session and
why/what happened.
In terms of changes that need to be made before the next pilot, one thing we noticed was that a lot of pilot
users were confused as to what the stars mean. Is having more stars a good thing or a bad thing? So, we
decided that we should add some colors (red indicating bad and green indicating good) as well as a scale.
In addition, we also realized that the text that we are currently displaying is still too text heavy, and we
should move that into a “Learn More” section along with one main bullet point or idea.
2. [+0.5] Involve another pilot user outside of the course. Include 0.5-1p of notes on this second
pilot user. Summarize in a few sentences: What happened? Why? What changes do you need to
make to your system before the next pilot?
We have kept these summaries in a separate document in our github folder.

## Before conducting an evaluation [+3]

**1. [+0.5] Articulate1-2 questions motivating the evaluation. In other words, what are the 1-2 things
you want to prioritize learning through the evaluation?**


- Our primary question remains: Do the people care? Or, is the system designed in the best possible
    way to get people to use the app?
       - This can be hard to evaluate for, but we want to design the system as optimally for
          engagement as we can.
- Our second question: Does this popup/app educate people about data privacy in a dynamic way?
    - There are many sub questions stemming from this, such as:
       - Is the information well organized?
       - Is it relevant and critical information?
       - Does it allow users to choose the depth of knowledge they can receive?
**2. [+0.5] What metrics will you use to answer the above research questions? Why are these metrics
appropriate? What are the benefits and drawbacks of using these metrics?
- Requirements: You are required to conduct a mixed-methods study where you collect
qualitative and quantitative data. In your response to this question, describe what kind of
data (e.g., open-ended survey, interview, time, clicks, etc.) will be useful for answering your
motivating questions.**
- Metrics:
- Time spent on each page of our tab: This is useful because it tells us how easily digestible
our information is
- This can be followed up with brief questions to check engagement with the
material
- This can be biased since we’re asking them to look at the popup so they HAVE to
spend time on each window, but it can still tell us how quickly they can scan and
understand the content on the page. If it takes them a long time, we know we
need to reformat
- Clicks on each link of the tab: This will tell us if our popup is interesting enough for users
to want to learn more.
- Coupled with time spent on each tab, this information can inform us what areas
need to be improved/made more engaging.
- Again, it is possible users could click many times because something is unclear
so we have to be careful how we interpret this, and it shold be be supported by
interview data
- Qualitative Interview data: This will be helpful again to gauge user impressions of the
popup/app. An important aspect of our popup is wether or not users enjoy interacting
with it. It is important to find out if it is easy to understand, keeps attention, and has clear
information.
- This method is appropriate since it is challenging to gauge this solely from a
user’s interactions with the popup. This method will provide highly beneficial
insights that would be hard to find another way.
- A drawback could be that users are less likely to be honest if our app is horrible
since they’d have to tell us that to our faces.... So perhaps following up with an
anonymous survey would also be beneficial.


**3. [+1] Specify a plan for recruiting participants.**
    **- How will you contact participants (e.g., mailing lists, in-person, etc)?**
    **- What are your inclusion/exclusion criteria for participants?**
    **- Will you include participants you interviewed for user research? Why or why not?**
    **- Where will you perform the evaluation?**
    **- What data will you collect from participants? How will you inform them of this and obtain**
       **informed consent?**
Recruitment:
    - We will recruit adults 18+
       - We will include both people who do and do not use social media
       - We’ll exclude people who we have already interviewed (see below for details)
    - We will contact participants in person, via social media posts, and via departmental mailing lists
    - We will exclude people who we originally interviewed since they are likely to have a more
       informed view of social media policies. Since policies were discussed during the interviews,
       participants have some prior knowledge. It will be beneficial to show to people who are at
       varying levels of knowledge regarding privacy policy of their own volition, as well as people who
       don’t know the motivation behind our project so as to gain a better understanding of if our goal is
       being met.
    - Evaluations will be performed in person with the test monitor screen being recorded as they use
       the popup.
    - We will collect this screen data, which does not include their video log or audio log but will
       include a recording of them navigating on the monitor. We will ask prior to recording for consent.
    - We will also record their interview data in audio recorded form with their verbal consent.
    - User names will not be kept or correlated with their user data and interview data
4. [ **+1] Write out a step-by-step protocol for conducting each user evaluation. Getting on the same
page is important for more easily conducting studies and analyzing data across participants. Your
protocol should include: (1) a script of what you will say to each participant; (2) what
behaviors/responses you expect from participants and how that may change the flow of the study, if
at all; and (3) how you will transition between phases of the study (e.g., from a task to an interview).**
Part 1: Context
    ● Starting Point: Chrome Browser w/o any open tabs, PrivacyPal extension loaded, logged out of
       all social media (TikTok, FB, Insta)
    ● Pick a social media platform for this Pilot (referenced as **SM** from now on)
    ● Set the Scene: Currently, you do not use **SM** but would like to make a new account because you
       have many friends on said app. However, you have long been skeptical of joining because you
       aren’t sure what they can do with your data. Go ahead and try to sign up for an account
Part 2: Pilot
    ● Observe what the user does in accordance with what our optimal flow is
       ○ User gets to sign up


○ User sees popup
○ User reads popup
○ Users understands important privacy policy markers as result of reading summaries +
ratings of each privacy policy metric
○ User closes popup
○ User continues with signup with better understanding of how their data is used
● Redirect user if they get off optimal flow and note down reason
○ If user can’t get to page when popup shows (signup page), move in and tell them to go to
said page
○ If user closes popup accidentally, reload site and let them know to wait
■ Plus note that maybe we should have an injected popup (into the DOM)
Part 3: Interview/Feedback
● Let user know that pilot is over
○ Thank them for participating
○ Ask if they would be fine answering a few questions about their experience
● Ask user for feedback on their experience and thoughts on the app idea as a whole
○ Ask about users’ thoughts on LLM summaries and ratings
■ Clarity?
■ Helpful
○ Ask about app idea
■ Would they use something like this?
■ General thoughts on better understanding their data privacy on apps
● Expected Feedback
○ LLM summaries a bit vague
○ What do ratings really mean?
○ Like the idea of better understanding what happens to their data with big companies
○ Easy to accidentally click off extension popup

## For fun [+0.5]

1. [+0.5] Name your system!
**Privacy Pal**
2. [+0.5] DEPTH: Design a logo for your system. Include a PNG in your repo. Add it to the
README.

![logo](../logo.png)

**Did you use a generative AI tool for this assignment? If so, which tool(s) and how?**
We did not use generative AI for this assignment.
**How much time did you spend on this assignment**
**as a group?**
**individually?**
    We each spent about 1 hour each for the writing part of this core assignment. That includes
writing up the “transcripts” for the pilots we did. We spent 2 hours in class on pilots together, then another
half hour each conducting a pilot by ourselves with different users. We also spent a lot of time developing
the code for our chrome extension. We would estimate the time required for the progress we made so far
at about 7 hours each for this week.
